{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import random\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "(1) Generate filtered dataset, export to CSV (DON'T NEED TO RUN)\n",
    "\n",
    "(2) Read the aggregated file with Reviews information only for food related venues of Phoenix\n",
    "\n",
    "(3) Import business data and filter by city and Filter data by city (Phoenix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DONT NEED TO RUN ### RUN ONLY IN THE FIRST TIME SINCE ITS TOO LONG\n",
    "## DONT NEED TO RUN #ListofBusinessID = Yelp_Business['business_id']\n",
    "## DONT NEED TO RUN #ListofBusinessID = ListofBusinessID.unique()\n",
    "## DONT NEED TO RUN #ListofBusinessID = ListofBusinessID.tolist()\n",
    "## DONT NEED TO RUN #Yelp_Reviews = pd.read_csv('ProjectFall2019/data/yelp_review.csv') \n",
    "## DONT NEED TO RUN #Yelp_Reviews.head()\n",
    "## DONT NEED TO RUN ## FILTER THE REVIEWS ONLY BY BUSINESS IN THE YELP_BUSINESS df (which has only Phoenix venues)\n",
    "## DONT NEED TO RUN #Yelp_Reviews = Yelp_Reviews[Yelp_Reviews['business_id'].isin(ListofBusinessID)]\n",
    "## DONT NEED TO RUN #Yelp_Reviews.to_csv('ProjectFall2019/data/Yelp_Reviews_Phoenix_Food.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AFTER THE ABOVE CODE WAS EXECUTED ONCE, WE CAN GET THE FILTERED DATA\n",
    "#Yelp_Reviews_Phoenix_Food = pd.read_csv('ProjectFall2019/data/Yelp_Reviwes_Phoenix_Food.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yelp_Business = pd.read_csv('data/yelp_business.csv') \n",
    "Yelp_Business = Yelp_Business.loc[Yelp_Business['city'] == \"Phoenix\"]\n",
    "#Yelp_Reviews_Phoenix_Food = Yelp_Reviews_Phoenix_Food.drop(columns=['Unnamed: 0'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ListofBusinessID = Yelp_Business['business_id']\n",
    "#ListofBusinessID = ListofBusinessID.unique()\n",
    "#ListofBusinessID = ListofBusinessID.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DONT NEED TO RUN ### RUN ONLY IN THE FIRST TIME SINCE ITS TOO LONG\n",
    "#Yelp_Reviews = pd.read_csv('ProjectFall2019/data/yelp_review.csv') \n",
    "## FILTER THE REVIEWS ONLY BY BUSINESS IN THE YELP_BUSINESS df (which has only Phoenix venues)\n",
    "#Yelp_Reviews = Yelp_Reviews[Yelp_Reviews['business_id'].isin(ListofBusinessID)]\n",
    "#Yelp_Reviews.to_csv('ProjectFall2019/data/Yelp_Reviews_Phoenix.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'data/Yelp_Reviwes_Phoenix.csv' does not exist: b'data/Yelp_Reviwes_Phoenix.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-7b448d08344e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mYelp_Reviews_Phoenix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'data/Yelp_Reviwes_Phoenix.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1135\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1136\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1917\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'data/Yelp_Reviwes_Phoenix.csv' does not exist: b'data/Yelp_Reviwes_Phoenix.csv'"
     ]
    }
   ],
   "source": [
    "Yelp_Reviews_Phoenix = pd.read_csv('data/Yelp_Reviwes_Phoenix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build categories list\n",
    "Just need to run this in the first time, to generate the categories list to be worked on MS Excel (DON'T NEED TO RUN)\n",
    "\n",
    "Executed this process to generate the initial list of categories to build the \"Project Categories\" on MS Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ListCategories = Yelp_Business['categories']\n",
    "#ListCategories = ListCategories.unique()\n",
    "#ListCategories = ListCategories.tolist()\n",
    "#with open('ProjectFall2019/ListOfCategories.csv', 'wb') as myfile:\n",
    "#    wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "#    wr.writerow(ListCategories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Excel file with categories\n",
    "(1) Import the generated Excel with the \"Yelp categories+subcategories\" and the \"Project Categories\"\n",
    "\n",
    "(2) Merge this information ino the YelpBusiness dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ListCategories_Shortlist = pd.read_excel('ProjectFall2019/data/ListOfCategories.xlsx', sheet_name='Shortlist', skip_blank_lines=True, encoding='utf-8', usecols = \"A,P\")\n",
    "#Yelp_Business = pd.merge(ListCategories_Shortlist, Yelp_Business, left_on=('ShortListed'), right_on=('categories'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListCategories = pd.read_excel('data/ListOfCategories.xlsx', sheet_name='AllCategories', skip_blank_lines=True, encoding='utf-8', usecols = \"A,B,X\")\n",
    "Yelp_Business = pd.merge(ListCategories, Yelp_Business, left_on=('FullList'), right_on=('categories'))\n",
    "Yelp_Business = Yelp_Business.drop(columns=['FullList'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Business/Categories list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListofBusinessCategories = pd.DataFrame()\n",
    "ListofBusinessCategories['business_id'] = Yelp_Business['business_id'] \n",
    "ListofBusinessCategories['ProjectCategory'] = Yelp_Business['ProjectCategory'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataframe into Traning / Testing / Validation\n",
    "\n",
    "Generate 3 separate datasets with the following info:\n",
    "\n",
    "- Review_id - `ex: Z7U7MMef6Tbj_ZbSFzLRUw`\n",
    "\n",
    "- User_id - `ex: lUQzbz84E30c4L-JbqE3FQ\t`\n",
    "\n",
    "- Business_id - `ex: 1JF9TbJ2d5hH8xsQvvklHg\t`\n",
    "\n",
    "- Stars - `ex: 4`\n",
    "\n",
    "- Date - `ex: 2016-04-12` \n",
    "\n",
    "- Text - `ex: This place is good, but not great.  I liked the outside patio and the burger was tasty.`\n",
    "\n",
    "- Usefull - `ex: 2`\n",
    "\n",
    "- Funny - `ex: 1` \n",
    "\n",
    "- Cool - `ex: 2`\n",
    "\n",
    "- Set - `ex: Training`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLIT IN TRAIN TESTE VALIDATION\n",
    "Yelp_Reviews_Phoenix_Food_TTV = pd.DataFrame()\n",
    "Yelp_Reviews_Phoenix_Food_TTV = Yelp_Reviews_Phoenix_Food.copy()\n",
    "Yelp_Reviews_Phoenix_Food_TTV.shape\n",
    "#### DEFINE THE TRAINING SET USING RANDOM\n",
    "# SELECT 80% AMONG THE INDEXES\n",
    "Yelp_Training = random.sample(xrange(0,len(Yelp_Reviews_Phoenix_Food_TTV)), int(len(Yelp_Reviews_Phoenix_Food_TTV.index)*0.8))\n",
    "# SET TRAINING SET\n",
    "Yelp_Reviews_Phoenix_Food_TTV.loc[Yelp_Training,'Set'] = 'Training'\n",
    "#### SELECT REMAINING DATA (20%)\n",
    "# SEPARATE THE REMAINING DATASET (EVERYTHING THAT IS NOT TRAINING)\n",
    "Remaining= Yelp_Reviews_Phoenix_Food_TTV.loc[Yelp_Reviews_Phoenix_Food_TTV['Set'] != 'Training']\n",
    "# SPLIT THE REMAINING INTO 2 PARTS (10%)\n",
    "Yelp_Testing = random.sample(Remaining.index.tolist(), int(len(Remaining.index)*0.5))\n",
    "# APPLY LABEL TESTING TO THE FIRST 10%\n",
    "Yelp_Reviews_Phoenix_Food_TTV.loc[Yelp_Testing,'Set'] = 'Testing'\n",
    "# APPLY LABEL VALIDATION TO THE REMAINING 10%\n",
    "Yelp_Reviews_Phoenix_Food_TTV['Set'].fillna('Validation', inplace=True)\n",
    "#### GENERATE TRAINING / TEST / VALIDATION\n",
    "Yelp_Reviews_Phoenix_Food_Training = Yelp_Reviews_Phoenix_Food_TTV[Yelp_Reviews_Phoenix_Food_TTV['Set']==\"Training\"].copy()\n",
    "Yelp_Reviews_Phoenix_Food_Testing = Yelp_Reviews_Phoenix_Food_TTV[Yelp_Reviews_Phoenix_Food_TTV['Set']==\"Testing\"].copy()\n",
    "Yelp_Reviews_Phoenix_Food_Validation = Yelp_Reviews_Phoenix_Food_TTV[Yelp_Reviews_Phoenix_Food_TTV['Set']==\"Validation\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Category and Postal Code into the Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yelp_Reviews_Phoenix_Food_Training = pd.merge(ListofBusinessCategories, Yelp_Reviews_Phoenix_Food_Training, on=('business_id'))\n",
    "Yelp_Reviews_Phoenix_Food_Training = pd.merge(Yelp_Reviews_Phoenix_Food_Training,Yelp_Business[['business_id','postal_code']],on='business_id', how='left')\n",
    "ListofBusiness_Training = Yelp_Reviews_Phoenix_Food_Training['business_id'] \n",
    "ListofBusiness_Training = ListofBusiness_Training.unique()\n",
    "ListofBusiness_Training = ListofBusiness_Training.tolist()\n",
    "\n",
    "Yelp_Reviews_Phoenix_Food_Testing = pd.merge(ListofBusinessCategories, Yelp_Reviews_Phoenix_Food_Testing, on=('business_id'))\n",
    "Yelp_Reviews_Phoenix_Food_Testing = pd.merge(Yelp_Reviews_Phoenix_Food_Testing,Yelp_Business[['business_id','postal_code']],on='business_id', how='left')\n",
    "ListofBusiness_Testing = Yelp_Reviews_Phoenix_Food_Testing['business_id'] \n",
    "ListofBusiness_Testing = ListofBusiness_Testing.unique()\n",
    "ListofBusiness_Testing = ListofBusiness_Testing.tolist()\n",
    "\n",
    "Yelp_Reviews_Phoenix_Food_Validation = pd.merge(ListofBusinessCategories, Yelp_Reviews_Phoenix_Food_Validation, on=('business_id'))\n",
    "Yelp_Reviews_Phoenix_Food_Validation = pd.merge(Yelp_Reviews_Phoenix_Food_Validation,Yelp_Business[['business_id','postal_code']],on='business_id', how='left')\n",
    "ListofBusiness_Validation = Yelp_Reviews_Phoenix_Food_Validation['business_id'] \n",
    "ListofBusiness_Validation = ListofBusiness_Validation.unique()\n",
    "ListofBusiness_Validation = ListofBusiness_Validation.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Pivot Table with information of # of restaurants per Postal Code\n",
    "(1) Filter Business dataframe to get only Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yelp_Business_Training = pd.DataFrame()\n",
    "Yelp_Business_Training['business_id'] = Yelp_Business['business_id']\n",
    "Yelp_Business_Training['postal_code'] = Yelp_Business['postal_code']\n",
    "Yelp_Business_Training['ProjectCategory'] = Yelp_Business['ProjectCategory']\n",
    "Yelp_Business_Training = Yelp_Business_Training[Yelp_Business_Training['business_id'].isin(ListofBusiness_Training)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Category_per_Zip_Freq = pd.DataFrame()\n",
    "Category_per_Zip_Freq['postal_code'] = Yelp_Business_Training['postal_code']\n",
    "Category_per_Zip_Freq['ProjectCategory'] = Yelp_Business_Training['ProjectCategory']\n",
    "\n",
    "#Category_per_Zip_Freq.to_csv('ProjectFall2019\\data\\Category_per_Zip_Freq_PRE.csv')\n",
    "sum_col = Category_per_Zip_Freq.groupby(['postal_code', 'ProjectCategory'])['ProjectCategory'].count() # don't reset the index!\n",
    "Category_per_Zip_Freq = Category_per_Zip_Freq.set_index(['postal_code', 'ProjectCategory']) # make the same index here\n",
    "Category_per_Zip_Freq['sum_col'] = sum_col\n",
    "Category_per_Zip_Freq = Category_per_Zip_Freq.reset_index() # to take the hierarchical index off again\n",
    "Category_per_Zip_Freq.drop_duplicates(keep = 'first', inplace = True)\n",
    "### PIVOT IT\n",
    "Category_per_Zip_Freq=pd.pivot_table(Category_per_Zip_Freq,index='postal_code',columns='ProjectCategory',values='sum_col',fill_value=0)\n",
    "Category_per_Zip_Freq.to_csv(\"ProjectFall2019/data/Category_per_Zip_Freq.csv\")\n",
    "\n",
    "\n",
    "Category_per_Zip_Freq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListofPostalCode = Yelp_Business['postal_code']\n",
    "ListofPostalCode = ListofPostalCode.unique()\n",
    "ListofPostalCode = ListofPostalCode.tolist()\n",
    "\n",
    "df_Zip_TopCategory = pd.DataFrame()\n",
    "for zip in ListofPostalCode:\n",
    "    #print(zip)\n",
    "    df_Zip_TopCategory = df_Zip_TopCategory.append({'Zip' : zip} , ignore_index=True)\n",
    "    df_PCategory = pd.DataFrame()\n",
    "    df_PCategory['ProjectCategory'] = Yelp_Business.loc[(Yelp_Business['postal_code']==zip), 'ProjectCategory']\n",
    "    sum = df_PCategory.groupby(['ProjectCategory'])['ProjectCategory'].count()\n",
    "    df_PCategory = df_PCategory.set_index(['ProjectCategory']) \n",
    "    df_PCategory['sum'] = sum\n",
    "    df_PCategory.drop_duplicates(keep=\"first\", inplace=True)\n",
    "    df_PCategory.sort_values(by='sum', ascending=False, inplace=True)\n",
    "    df_PCategory['Category'] = df_PCategory.index\n",
    "    Size = df_PCategory.shape[0]\n",
    "    for i in range(0,Size):\n",
    "        Position = pd.DataFrame()\n",
    "        Position = df_PCategory.iloc[i].copy()\n",
    "        df_Zip_TopCategory.loc[df_Zip_TopCategory['Zip'] == zip, 'Top'+str(i+1)] = Position['Category']\n",
    "df_Zip_TopCategory"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython2",
  "version": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
